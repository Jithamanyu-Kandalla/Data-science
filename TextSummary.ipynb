{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiGikm1T6rJs",
        "outputId": "13a375a0-1023-4d74-a90b-d5a9b65d7247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extractive Summary: Artificial intelligence refers to the simulation of human intelligence in machines\n",
            "that are programmed to think like humans and mimic their actions. AI is being\n",
            "applied in a wide range of industries, from healthcare to finance, and has the\n",
            "potential to improve efficiency and decision-making.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#Extractive Summary method\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import heapq\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def simple_summarize(text, num_sentences=2):\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Tokenize words and remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text.lower())\n",
        "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "\n",
        "    # Calculate word frequencies\n",
        "    freq = FreqDist(words)\n",
        "\n",
        "    # Score sentences based on word frequencies\n",
        "    sentence_scores = {}\n",
        "    for sentence in sentences:\n",
        "        for word in word_tokenize(sentence.lower()):\n",
        "            if word in freq:\n",
        "                if sentence not in sentence_scores:\n",
        "                    sentence_scores[sentence] = freq[word]\n",
        "                else:\n",
        "                    sentence_scores[sentence] += freq[word]\n",
        "\n",
        "    # Get the top N sentences\n",
        "    summary_sentences = heapq.nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
        "\n",
        "    # Join the top sentences\n",
        "    summary = ' '.join(summary_sentences)\n",
        "\n",
        "    return summary\n",
        "\n",
        "# The text to summarize\n",
        "text = \"\"\"Artificial intelligence refers to the simulation of human intelligence in machines\n",
        "that are programmed to think like humans and mimic their actions. AI is being\n",
        "applied in a wide range of industries, from healthcare to finance, and has the\n",
        "potential to improve efficiency and decision-making.\"\"\"\n",
        "\n",
        "# Generate the summary\n",
        "summary = simple_summarize(text)\n",
        "print(\"Extractive Summary:\", summary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Abstractive Summary method\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "import torch\n",
        "\n",
        "def abstractive_summarize(text, max_length=60, min_length=10):\n",
        "    # Load pre-trained model and tokenizer\n",
        "    model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "    tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "    # Encode the text\n",
        "    inputs = tokenizer([text], max_length=1024, return_tensors='pt', truncation=True)\n",
        "\n",
        "    # Generate summary\n",
        "    summary_ids = model.generate(inputs['input_ids'],\n",
        "                                 num_beams=4,\n",
        "                                 max_length=max_length,\n",
        "                                 min_length=min_length,\n",
        "                                 length_penalty=2.0,\n",
        "                                 early_stopping=True)\n",
        "\n",
        "    # Decode the summary\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return summary\n",
        "\n",
        "# The text to summarize\n",
        "text = \"\"\"Artificial intelligence refers to the simulation of human intelligence in machines\n",
        "that are programmed to think like humans and mimic their actions. AI is being\n",
        "applied in a wide range of industries, from healthcare to finance, and has the\n",
        "potential to improve efficiency and decision-making.\"\"\"\n",
        "\n",
        "# Generate the summary\n",
        "summary = abstractive_summarize(text)\n",
        "print(\"Abstractive Summary:\", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPzSsJyA7CBQ",
        "outputId": "6eb4d546-9c98-47da-c234-799e13cf727d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abstractive Summary: Artificial intelligence refers to the simulation of human intelligence in machinesthat are programmed to think like humans and mimic their actions. AI is being applied in a wide range of industries, from healthcare to finance, and has the potential to improve efficiency and decision-making.\n"
          ]
        }
      ]
    }
  ]
}